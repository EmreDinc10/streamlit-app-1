{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30638165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bar\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Basic connection example.\n",
    "\"\"\"\n",
    "\n",
    "from base64 import decode\n",
    "import redis\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "r = redis.Redis(\n",
    "    host=os.getenv(\"REDIS_HOST\"),\n",
    "    port=int(os.getenv(\"REDIS_PORT\", \"6379\")),\n",
    "    username=os.getenv(\"REDIS_USERNAME\") or None,\n",
    "    password=os.getenv(\"REDIS_PASSWORD\"),\n",
    "    decode_responses=True\n",
    ")\n",
    "\n",
    "success = r.set('foo', 'bar')\n",
    "# True\n",
    "\n",
    "result = r.get('foo')\n",
    "print(result)\n",
    "# >>> bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5747da3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': 'redis vector search tutorial',\n",
       "  'title': 'Redis Vector Store | ü¶úÔ∏èüîó LangChain',\n",
       "  'snippet': \"Redis Vector Store\\nThis notebook covers how to get started with the Redis vector store.\\nRedis is a popular open-source, in-memory data structure store that can be used as a database, cache, message broker, and queue. It now includes vector similarity search capabilities, making it suitable for use as a vector store.\\nWhat is Redis?\\nMost developers are familiar with Redis\\n. At its core, Redis\\nis a NoSQL Database in the key-value family that can used as a cache, message broker, stream processing and a primary database. Developers choose Redis\\nbecause it is fast, has a large ecosystem of client libraries, and has been deployed by major enterprises for years.\\nOn top of these traditional use cases, Redis\\nprovides additional capabilities like the Search and Query capability that allows users to create secondary index structures within Redis\\n. This allows Redis\\nto be a Vector Database, at the speed of a cache.\\nRedis as a Vector Database\\nRedis\\nuses compressed, inverted indexes for fast indexing with a low memory footprint. It also supports a number of advanced features such as:\\n- Indexing of multiple fields in Redis hashes and\\nJSON\\n- Vector similarity search (with\\nHNSW\\n(ANN) orFLAT\\n(KNN)) - Vector Range Search (e.g. find all vectors within a radius of a query vector)\\n- Incremental indexing without performance loss\\n- Document ranking (using tf-idf, with optional user-provided weights)\\n- Field weighting\\n- Complex boolean queries with\\nAND\\n,OR\\n, andNOT\\noperators - Prefix matching, fuzzy matching, and exact-phrase queries\\n- Support for double-metaphone phonetic matching\\n- Auto-complete suggestions (with fuzzy prefix suggestions)\\n- Stemming-based query expansion in many languages (using Snowball)\\n- Support for Chinese-language tokenization and querying (using Friso)\\n- Numeric filters and ranges\\n- Geospatial searches using Redis geospatial indexing\\n- A powerful aggregations engine\\n- Supports for all\\nutf-8\\nencoded text - Retrieve full documents, selected fields, or only the document IDs\\n- Sorting results (for example, by creation date)\\nClients\\nSince Redis\\nis much more than just a vector database, there are often use cases that demand the usage of a Redis\\nclient besides just the LangChain\\nintegration. You can use any standard Redis\\nclient library to run Search and Query commands, but it's easiest to use a library that wraps the Search and Query API. Below are a few examples, but you can find more client libraries here.\\nDeployment options\\nThere are many ways to deploy Redis with RediSearch. The easiest way to get started is to use Docker, but there are are many potential options for deployment such as\\n- Redis Cloud\\n- Docker (Redis Stack)\\n- Cloud marketplaces: AWS Marketplace, Google Marketplace, or Azure Marketplace\\n- On-premise: Redis Enterprise Software\\n- Kubernetes: Redis Enterprise Software on Kubernetes\\nRedis connection Url schemas\\nValid Redis Url schemas are:\\nredis://\\n- Connection to Redis standalone, unencryptedrediss://\\n- Connection to Redis standalone, with TLS encryptionredis+sentinel://\\n- Connection to Redis server via Redis Sentinel, unencryptedrediss+sentinel://\\n- Connection to Redis server via Redis Sentinel, both connections with TLS encryption\\nMore information about additional connection parameters can be found in the redis-py documentation.\\nSetup\\nTo use the RedisVectorStore, you'll need to install the langchain-redis\\npartner package, as well as the other packages used throughout this notebook.\\n%pip install -qU langchain-redis langchain-huggingface sentence-transformers scikit-learn\\nNote: you may need to restart the kernel to use updated packages.\\nCredentials\\nRedis connection credentials are passed as part of the Redis Connection URL. Redis Connection URLs are versatile and can accommodate various Redis server topologies and authentication methods. These URLs follow a specific format that includes the connection protocol, authentication details, host, port, and database information. The basic structure of a Redis Connection URL i\",\n",
       "  'domain': 'python.langchain.com',\n",
       "  'lang': 'en',\n",
       "  'fetched_at': 1755019724,\n",
       "  'metadata': 'source=google;url=https://python.langchain.com/docs/integrations/vectorstores/redis',\n",
       "  'embedding': b''},\n",
       " {'query': 'redis vector search tutorial',\n",
       "  'title': 'Efficient vector similarity search with Redis: a step-by-step tutorial tutorial',\n",
       "  'snippet': 'Efficient vector similarity search with Redis: a step-by-step tutorial\\nEnhancing search results with vector embeddings and Redis\\nThe ability to search for information is crucial in today\\'s digital landscape, with users expecting search functionality in nearly every application and website. To improve search results, architects and developers must continuously explore new methods and architectures. One of the approaches is to utilize vector embeddings generated by deep learning models, which can enhance the accuracy and relevance of search results.\\nTo this end, many organizations are leveraging indexing techniques to transform their data into vector space. By representing data as vectors, it becomes possible to perform similarity searches that return the most relevant results.\\nIn this tutorial, we will explore how deep learning models can be used to create vector embeddings, which can then be indexed for efficient and accurate search with the help of Redis. With a thorough understanding of this approach, architects and developers can better appreciate the potential of AI-powered search capabilities and find best ways to improve the search experience for users.\\nWe will go through the process of:\\n- creating vector embeddings for Amazon product dataset,\\n- indexing them with Redis\\n- searching for similar vectors.\\nWe will also explore the pros and cons of different indexing methods and how they can be used to improve search performance.\\nLet\\'s get started!\\nCreate a new directory and create a new Jupyter notebook. Get the dataset CSV file from here. Store it in ./data/\\ndir. We will be using Python 3.8. Install the following\\ndependencies in the first cell:\\n%pip install redis numpy pandas\\n%pip install -U sentence-transformers\\nAfter installing the dependencies, you can start by importing the necessary libraries and defining any necessary classes or functions. In this case, we can import the following libraries and define a color class for later use:\\nimport random\\nimport numpy as np\\nimport pandas as pd\\nimport time\\nfrom redis import Redis\\nfrom redis.commands.search.field import VectorField\\nfrom redis.commands.search.field import TextField\\nfrom redis.commands.search.field import TagField\\nfrom redis.commands.search.query import Query\\nfrom redis.commands.search.result import Result\\nclass color:\\nPURPLE = \\'\\\\033[95m\\'\\nCYAN = \\'\\\\033[96m\\'\\nDARKCYAN = \\'\\\\033[36m\\'\\nBLUE = \\'\\\\033[94m\\'\\nGREEN = \\'\\\\033[92m\\'\\nYELLOW = \\'\\\\033[93m\\'\\nRED = \\'\\\\033[91m\\'\\nBOLD = \\'\\\\033[1m\\'\\nUNDERLINE = \\'\\\\033[4m\\'\\nEND = \\'\\\\033[0m\\'\\nThe Redis library is imported to interact with Redis, an in-memory data structure store often used as a database, cache, and message broker. We also import the following classes from redis.commands.search.field\\nand redis.commands.search.query\\nmodules:\\nVectorField\\n: used to represent vector fields in Redis, such as embeddings.TextField\\n: used to represent text fields in Redis.TagField\\n: used to represent tag fields in Redis.Query\\n: used to create search queries for Redis.Result\\n: used to represent search results returned by Redis.\\nIn addition, we define a color class that can be used to print colored text to the console. The color class has several attributes, such as PURPLE, CYAN, BOLD, etc., which can be used to colorize text output in the console.\\nThe next step involves loading Amazon product data into a Pandas DataFrame, and truncating long text fields to a maximum length of 512 characters, which is the maximum length supported by the pre-trained sentence embedding generator that we will be using later on.\\nHere\\'s the cell code to load the product data and truncate long text fields:\\nMAX_TEXT_LENGTH = 512\\nNUMBER_PRODUCTS = 1000\\n# define a function to auto-truncate long text fields\\ndef auto_truncate(val):\\nreturn val[:MAX_TEXT_LENGTH]\\n# load the product data and truncate long text fields\\nall_prods_df = pd.read_csv(\"data/product_data.csv\", converters={\\'bullet_point\\': auto_truncate, \\'item_keywords\\': auto_truncate, \\'item_name\\': auto_truncate})\\nall_prods_df[\\'primary_key\\']',\n",
       "  'domain': 'lablab.ai',\n",
       "  'lang': 'en',\n",
       "  'fetched_at': 1755019724,\n",
       "  'metadata': 'source=google;url=https://lablab.ai/t/efficient-vector-similarity-search-with-redis-a-step-by-step-tutorial',\n",
       "  'embedding': b''}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Key-free web search + fetch + extract (googlesearch-python + httpx + trafilatura)\n",
    "# Adds: language detection, canonical URL normalization, de-duplication, and gated-page filtering.\n",
    "# Overfetch set to k*5; gated length threshold = 1200 chars.\n",
    "# pip install googlesearch-python httpx trafilatura lxml langdetect\n",
    "\n",
    "import time, re\n",
    "from typing import List, Dict, Any\n",
    "import httpx\n",
    "from googlesearch import search\n",
    "import trafilatura\n",
    "from lxml import html as lxml_html\n",
    "from langdetect import detect_langs\n",
    "from urllib.parse import urlsplit, urlunsplit, parse_qsl, urlencode\n",
    "\n",
    "UA = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36\"\n",
    "\n",
    "# ---- helpers ----\n",
    "def _domain(u: str) -> str:\n",
    "    return re.sub(r\"^https?://(www\\.)?([^/:]+).*$\", r\"\\2\", u)\n",
    "\n",
    "def _strip_tags(html_str: str) -> str:\n",
    "    try:\n",
    "        root = lxml_html.fromstring(html_str)\n",
    "        return \"\\n\".join(s.strip() for s in root.xpath(\"//text()\") if s.strip())\n",
    "    except Exception:\n",
    "        return re.sub(r\"<[^>]+>\", \" \", html_str)\n",
    "\n",
    "def _html_title(html_str: str) -> str:\n",
    "    try:\n",
    "        root = lxml_html.fromstring(html_str)\n",
    "        t = (root.xpath('string(//title)') or '').strip()\n",
    "        if not t:\n",
    "            t = (root.xpath('string(//h1[1])') or '').strip()\n",
    "        if not t:\n",
    "            metas = root.xpath('//meta[@property=\"og:title\"]/@content | //meta[@name=\"twitter:title\"]/@content')\n",
    "            t = (metas[0] if metas else '').strip()\n",
    "        return re.sub(r'\\s+', ' ', t)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def _canonical_from_html(html_str: str) -> str:\n",
    "    try:\n",
    "        root = lxml_html.fromstring(html_str)\n",
    "        hrefs = root.xpath('//link[translate(@rel,\"CANONICAL\",\"canonical\")=\"canonical\"]/@href')\n",
    "        return hrefs[0].strip() if hrefs else \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "_TRACKING_PARAMS = {\n",
    "    \"utm_source\",\"utm_medium\",\"utm_campaign\",\"utm_term\",\"utm_content\",\n",
    "    \"utm_id\",\"gclid\",\"fbclid\",\"mc_cid\",\"mc_eid\",\"igshid\",\"ref\",\"mkt_tok\"\n",
    "}\n",
    "\n",
    "def _normalize_url(u: str, html_str: str = \"\") -> str:\n",
    "    canon = _canonical_from_html(html_str)\n",
    "    if canon and canon.startswith(\"http\"):\n",
    "        u = canon\n",
    "    parts = urlsplit(u)\n",
    "    scheme = parts.scheme.lower() if parts.scheme else \"http\"\n",
    "    netloc = parts.netloc.lower()\n",
    "    if netloc.endswith(\":80\"):  netloc = netloc[:-3]\n",
    "    if netloc.endswith(\":443\"): netloc = netloc[:-4]\n",
    "    path = re.sub(r\"/{2,}\", \"/\", parts.path or \"/\")\n",
    "    qpairs = [(k, v) for (k, v) in parse_qsl(parts.query, keep_blank_values=True) if k not in _TRACKING_PARAMS]\n",
    "    query = urlencode(qpairs, doseq=True)\n",
    "    fragment = \"\"\n",
    "    if path != \"/\" and path.endswith(\"/\"):\n",
    "        path = path[:-1]\n",
    "    return urlunsplit((scheme, netloc, path, query, fragment))\n",
    "\n",
    "def _detect_lang(text: str) -> str:\n",
    "    try:\n",
    "        langs = detect_langs(text[:4000])\n",
    "        if not langs: return \"\"\n",
    "        best = max(langs, key=lambda l: l.prob)\n",
    "        return best.lang if best.prob >= 0.6 else \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def _looks_gated(text: str, html: str) -> bool:\n",
    "    blob = f\"{text}\\n{html}\".lower()\n",
    "    bad_markers = [\n",
    "        \"access denied\", \"requires authorization\", \"sign in\", \"sign-in\", \"log in\", \"login\",\n",
    "        \"subscribe to read\", \"subscription required\", \"paywall\", \"403 forbidden\",\n",
    "        \"captcha\", \"enable javascript\", \"are you a robot\", \"request blocked\", \"not authorized\"\n",
    "    ]\n",
    "    if any(m in blob for m in bad_markers):\n",
    "        return True\n",
    "    # \"Soft 404\": very short or boilerplate-heavy pages\n",
    "    return len(text.strip()) < 1200  # bumped threshold\n",
    "\n",
    "def _extract(html: str, url: str) -> Dict[str, str]:\n",
    "    txt = trafilatura.extract(\n",
    "        html, url=url,\n",
    "        include_comments=False,\n",
    "        include_tables=False,\n",
    "        favor_recall=True\n",
    "    )\n",
    "    if not txt:\n",
    "        txt = _strip_tags(html) or \"\"\n",
    "    title = \"\"\n",
    "    for line in (txt.splitlines()[:10] if txt else []):\n",
    "        if line.lower().startswith(\"title:\"):\n",
    "            title = line.split(\":\", 1)[1].strip()\n",
    "            break\n",
    "    if not title:\n",
    "        title = _html_title(html)\n",
    "    if not title:\n",
    "        slug = re.sub(r'[?#].*$', '', url).rstrip('/').rsplit('/', 1)[-1]\n",
    "        title = slug.replace('-', ' ').replace('_', ' ').strip()\n",
    "    return {\"title\": title, \"text\": (txt or \"\").strip()}\n",
    "\n",
    "# ---- main search ----\n",
    "def web_search_google(query: str, k: int = 5, timeout: int = 15,\n",
    "                      dedupe_by_domain: bool = True) -> List[Dict[str, Any]]:\n",
    "    # 1) Overfetch Google results (k*5) for better post-filtering\n",
    "    urls = list(search(query, num_results=max(k*5, k)))\n",
    "    if not urls:\n",
    "        return []\n",
    "\n",
    "    # 2) Fetch pages (cap downloads to k*5 as well)\n",
    "    pages = []\n",
    "    with httpx.Client(follow_redirects=True, headers={\"User-Agent\": UA}) as client:\n",
    "        for u in urls:\n",
    "            if len(pages) >= max(k*5, k):\n",
    "                break\n",
    "            try:\n",
    "                r = client.get(u, timeout=timeout)\n",
    "                r.raise_for_status()\n",
    "                pages.append((u, r.text))\n",
    "            except Exception:\n",
    "                pages.append((u, \"\"))\n",
    "\n",
    "    # 3) Extract, normalize, detect language; drop gated/soft-404s\n",
    "    now = int(time.time())\n",
    "    items = []\n",
    "    for u, h in pages:\n",
    "        ex = _extract(h, u)\n",
    "        canon = _normalize_url(u, h)\n",
    "        text = ex[\"text\"]\n",
    "        if _looks_gated(text, h):\n",
    "            continue\n",
    "        lang = _detect_lang(text) if text else \"\"\n",
    "        items.append({\n",
    "            \"query\": query,\n",
    "            \"title\": ex[\"title\"],\n",
    "            \"snippet\": (text[:4000]).strip(),\n",
    "            \"domain\": _domain(canon),\n",
    "            \"lang\": lang,\n",
    "            \"fetched_at\": now,\n",
    "            \"metadata\": f\"source=google;url={canon}\",\n",
    "            \"embedding\": b\"\",\n",
    "            \"url\": canon,\n",
    "            \"_len\": len(text)\n",
    "        })\n",
    "\n",
    "    if not items:\n",
    "        return []\n",
    "\n",
    "    # 4) De-duplicate by canonical URL first, then optionally by domain\n",
    "    unique_by_url = {}\n",
    "    for it in sorted(items, key=lambda x: x[\"_len\"], reverse=True):\n",
    "        if it[\"url\"] not in unique_by_url:\n",
    "            unique_by_url[it[\"url\"]] = it\n",
    "    deduped = list(unique_by_url.values())\n",
    "\n",
    "    if dedupe_by_domain:\n",
    "        winner_by_domain = {}\n",
    "        for it in deduped:\n",
    "            d = it[\"domain\"]\n",
    "            if d not in winner_by_domain or it[\"_len\"] > winner_by_domain[d][\"_len\"]:\n",
    "                winner_by_domain[d] = it\n",
    "        deduped = list(winner_by_domain.values())\n",
    "\n",
    "    # 5) Trim helpers, sort by snippet length, and return top-k\n",
    "    for it in deduped:\n",
    "        it.pop(\"_len\", None)\n",
    "        it.pop(\"url\", None)\n",
    "    deduped.sort(key=lambda x: len(x[\"snippet\"]), reverse=True)\n",
    "    return deduped[:k]\n",
    "\n",
    "# --- sample run in notebook ---\n",
    "docs = web_search_google(\"redis vector search tutorial\", k=5)\n",
    "docs[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b6948d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index_name': 'websearch_cache_idx', 'index_options': [], 'index_definition': ['key_type', 'JSON', 'prefixes', ['search_cache:'], 'default_score', '1', 'indexes_all', 'false'], 'attributes': [['identifier', '$.query', 'attribute', 'query', 'type', 'TEXT', 'WEIGHT', '1', 'SORTABLE', 'UNF', 'NOSTEM'], ['identifier', '$.title', 'attribute', 'title', 'type', 'TEXT', 'WEIGHT', '1'], ['identifier', '$.snippet', 'attribute', 'snippet', 'type', 'TEXT', 'WEIGHT', '1'], ['identifier', '$.domain', 'attribute', 'domain', 'type', 'TAG', 'SEPARATOR', ','], ['identifier', '$.lang', 'attribute', 'lang', 'type', 'TAG', 'SEPARATOR', ','], ['identifier', '$.fetched_at', 'attribute', 'fetched_at', 'type', 'NUMERIC', 'SORTABLE', 'UNF'], ['identifier', '$.metadata', 'attribute', 'metadata', 'type', 'TEXT', 'WEIGHT', '1'], ['identifier', '$.embedding', 'attribute', 'embedding', 'type', 'VECTOR', 'algorithm', 'HNSW', 'data_type', 'FLOAT32', 'dim', 1536, 'distance_metric', 'COSINE', 'M', 16, 'ef_construction', 200]], 'num_docs': 0, 'max_doc_id': 0, 'num_terms': 0, 'num_records': 0, 'inverted_sz_mb': '0', 'vector_index_sz_mb': '0', 'total_inverted_index_blocks': 0, 'offset_vectors_sz_mb': '0', 'doc_table_size_mb': '0.01532745361328125', 'sortable_values_size_mb': '0', 'key_table_size_mb': '0', 'tag_overhead_sz_mb': '0', 'text_overhead_sz_mb': '0', 'total_index_memory_sz_mb': '0.01532745361328125', 'geoshapes_sz_mb': '0', 'records_per_doc_avg': 'nan', 'bytes_per_record_avg': 'nan', 'offsets_per_term_avg': 'nan', 'offset_bits_per_record_avg': 'nan', 'hash_indexing_failures': 0, 'total_indexing_time': '0', 'indexing': 0, 'percent_indexed': '1', 'number_of_uses': 4, 'cleaning': 0, 'gc_stats': ['bytes_collected', '0', 'total_ms_run', '0', 'total_cycles', '0', 'average_cycle_time_ms', 'nan', 'last_run_time_ms', '0', 'gc_numeric_trees_missed', '0', 'gc_blocks_denied', '0'], 'cursor_stats': ['global_idle', 0, 'global_total', 0, 'index_capacity', 128, 'index_total', 0], 'dialect_stats': ['dialect_1', 0, 'dialect_2', 0, 'dialect_3', 0, 'dialect_4', 0], 'Index Errors': ['indexing failures', 0, 'last indexing error', 'N/A', 'last indexing error key', 'N/A', 'background indexing status', 'OK'], 'field statistics': [['identifier', '$.query', 'attribute', 'query', 'Index Errors', ['indexing failures', 0, 'last indexing error', 'N/A', 'last indexing error key', 'N/A']], ['identifier', '$.title', 'attribute', 'title', 'Index Errors', ['indexing failures', 0, 'last indexing error', 'N/A', 'last indexing error key', 'N/A']], ['identifier', '$.snippet', 'attribute', 'snippet', 'Index Errors', ['indexing failures', 0, 'last indexing error', 'N/A', 'last indexing error key', 'N/A']], ['identifier', '$.domain', 'attribute', 'domain', 'Index Errors', ['indexing failures', 0, 'last indexing error', 'N/A', 'last indexing error key', 'N/A']], ['identifier', '$.lang', 'attribute', 'lang', 'Index Errors', ['indexing failures', 0, 'last indexing error', 'N/A', 'last indexing error key', 'N/A']], ['identifier', '$.fetched_at', 'attribute', 'fetched_at', 'Index Errors', ['indexing failures', 0, 'last indexing error', 'N/A', 'last indexing error key', 'N/A']], ['identifier', '$.metadata', 'attribute', 'metadata', 'Index Errors', ['indexing failures', 0, 'last indexing error', 'N/A', 'last indexing error key', 'N/A']], ['identifier', '$.embedding', 'attribute', 'embedding', 'Index Errors', ['indexing failures', 0, 'last indexing error', 'N/A', 'last indexing error key', 'N/A'], 'memory', 0, 'marked_deleted', 0]]}\n"
     ]
    }
   ],
   "source": [
    "print(r.ft(\"websearch_cache_idx\").info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "951761d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emredinc/projects/streamlit-app-1/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedder = SentenceTransformer('msmarco-distilbert-base-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50a42351",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "Index already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResponseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m      6\u001b[39m schema = (\n\u001b[32m      7\u001b[39m     TextField(\u001b[33m\"\u001b[39m\u001b[33m$.query\u001b[39m\u001b[33m\"\u001b[39m,   as_name=\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m,   no_stem=\u001b[38;5;28;01mTrue\u001b[39;00m, sortable=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m      8\u001b[39m     TextField(\u001b[33m\"\u001b[39m\u001b[33m$.title\u001b[39m\u001b[33m\"\u001b[39m,   as_name=\u001b[33m\"\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     }, as_name=\u001b[33m\"\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m )\n\u001b[32m     23\u001b[39m definition = IndexDefinition(prefix=[\u001b[33m\"\u001b[39m\u001b[33msearch_cache:\u001b[39m\u001b[33m\"\u001b[39m], index_type=IndexType.JSON)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mft\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINDEX_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefinition\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefinition\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/streamlit-app-1/venv/lib/python3.13/site-packages/redis/commands/search/commands.py:222\u001b[39m, in \u001b[36mSearchCommands.create_index\u001b[39m\u001b[34m(self, fields, no_term_offsets, no_field_flags, stopwords, definition, max_text_fields, temporary, no_highlight, no_term_frequencies, skip_initial_scan)\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    220\u001b[39m     args += fields.redis_args()\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/streamlit-app-1/venv/lib/python3.13/site-packages/redis/client.py:621\u001b[39m, in \u001b[36mRedis.execute_command\u001b[39m\u001b[34m(self, *args, **options)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **options):\n\u001b[32m--> \u001b[39m\u001b[32m621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/streamlit-app-1/venv/lib/python3.13/site-packages/redis/client.py:632\u001b[39m, in \u001b[36mRedis._execute_command\u001b[39m\u001b[34m(self, *args, **options)\u001b[39m\n\u001b[32m    630\u001b[39m     \u001b[38;5;28mself\u001b[39m.single_connection_lock.acquire()\n\u001b[32m    631\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m632\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_command_parse_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_close_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._single_connection_client:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/streamlit-app-1/venv/lib/python3.13/site-packages/redis/retry.py:105\u001b[39m, in \u001b[36mRetry.call_with_retry\u001b[39m\u001b[34m(self, do, fail)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m._supported_errors \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m    107\u001b[39m         failures += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/streamlit-app-1/venv/lib/python3.13/site-packages/redis/client.py:633\u001b[39m, in \u001b[36mRedis._execute_command.<locals>.<lambda>\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    630\u001b[39m     \u001b[38;5;28mself\u001b[39m.single_connection_lock.acquire()\n\u001b[32m    631\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    632\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m conn.retry.call_with_retry(\n\u001b[32m--> \u001b[39m\u001b[32m633\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_command_parse_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    636\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m _: \u001b[38;5;28mself\u001b[39m._close_connection(conn),\n\u001b[32m    637\u001b[39m     )\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._single_connection_client:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/streamlit-app-1/venv/lib/python3.13/site-packages/redis/client.py:604\u001b[39m, in \u001b[36mRedis._send_command_parse_response\u001b[39m\u001b[34m(self, conn, command_name, *args, **options)\u001b[39m\n\u001b[32m    600\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    601\u001b[39m \u001b[33;03mSend a command and parse the response\u001b[39;00m\n\u001b[32m    602\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    603\u001b[39m conn.send_command(*args, **options)\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/streamlit-app-1/venv/lib/python3.13/site-packages/redis/client.py:651\u001b[39m, in \u001b[36mRedis.parse_response\u001b[39m\u001b[34m(self, connection, command_name, **options)\u001b[39m\n\u001b[32m    649\u001b[39m         options.pop(NEVER_DECODE)\n\u001b[32m    650\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m         response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ResponseError:\n\u001b[32m    653\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m EMPTY_RESPONSE \u001b[38;5;129;01min\u001b[39;00m options:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/streamlit-app-1/venv/lib/python3.13/site-packages/redis/connection.py:672\u001b[39m, in \u001b[36mAbstractConnection.read_response\u001b[39m\u001b[34m(self, disable_decoding, disconnect_on_error, push_request)\u001b[39m\n\u001b[32m    670\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, ResponseError):\n\u001b[32m    671\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m672\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m response\n\u001b[32m    673\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    674\u001b[39m         \u001b[38;5;28;01mdel\u001b[39;00m response  \u001b[38;5;66;03m# avoid creating ref cycles\u001b[39;00m\n",
      "\u001b[31mResponseError\u001b[39m: Index already exists"
     ]
    }
   ],
   "source": [
    "from redis.commands.search.index_definition import IndexDefinition, IndexType\n",
    "from redis.commands.search.field import TextField, TagField, NumericField, VectorField\n",
    "\n",
    "INDEX_NAME = \"websearch_cache_idx\"\n",
    "\n",
    "schema = (\n",
    "    TextField(\"$.query\",   as_name=\"query\",   no_stem=True, sortable=True),\n",
    "    TextField(\"$.title\",   as_name=\"title\"),\n",
    "    TextField(\"$.snippet\", as_name=\"snippet\"),\n",
    "    TagField(\"$.domain\",   as_name=\"domain\"),\n",
    "    TagField(\"$.lang\",     as_name=\"lang\"),\n",
    "    NumericField(\"$.fetched_at\", as_name=\"fetched_at\", sortable=True),\n",
    "    TextField(\"$.metadata\", as_name=\"metadata\"),\n",
    "    VectorField(\"$.embedding\", \"HNSW\", {\n",
    "        \"TYPE\": \"FLOAT32\",\n",
    "        \"DIM\": 1536,               # set to your embedding dimension\n",
    "        \"DISTANCE_METRIC\": \"COSINE\",\n",
    "        \"M\": 16,\n",
    "        \"EF_CONSTRUCTION\": 200\n",
    "    }, as_name=\"embedding\")\n",
    ")\n",
    "\n",
    "definition = IndexDefinition(prefix=[\"search_cache:\"], index_type=IndexType.JSON)\n",
    "\n",
    "r.ft(INDEX_NAME).create_index(schema, definition=definition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb248f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:5: RuntimeWarning: coroutine 'web_search_simple.<locals>.run' was never awaited\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! I noticed you entered \"<question, query>\" but I'm not sure what you‚Äôd like to know. Could you please provide more details or clarify your question so I can better assist you?\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "openai_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "\n",
    "client_o3 = AzureOpenAI(\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    azure_endpoint=openai_endpoint,  # predefined\n",
    "    api_key=openai_key              # predefined\n",
    ")\n",
    "\n",
    "resp = client_o3.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \n",
    "        \"\"\"\n",
    "\t\t\t\t\t<System prompt>\n",
    "\t\t\t\t\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": \"<question, query>\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(resp.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c37bbcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OK'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Recreate index with DIM=768  (only if you accept dropping the old one)\n",
    "from redis.commands.search.index_definition import IndexDefinition, IndexType\n",
    "from redis.commands.search.field import TextField, TagField, NumericField, VectorField\n",
    "\n",
    "INDEX_NAME = \"websearch_cache_idx\"\n",
    "r.ft(INDEX_NAME).dropindex(delete_documents=False)\n",
    "\n",
    "schema = (\n",
    "    TextField(\"$.query\",   as_name=\"query\",   no_stem=True, sortable=True),\n",
    "    TextField(\"$.title\",   as_name=\"title\"),\n",
    "    TextField(\"$.snippet\", as_name=\"snippet\"),\n",
    "    TagField(\"$.domain\",   as_name=\"domain\"),\n",
    "    TagField(\"$.lang\",     as_name=\"lang\"),\n",
    "    NumericField(\"$.fetched_at\", as_name=\"fetched_at\", sortable=True),\n",
    "    TextField(\"$.metadata\", as_name=\"metadata\"),\n",
    "    VectorField(\"$.embedding\", \"HNSW\", {\n",
    "        \"TYPE\": \"FLOAT32\",\n",
    "        \"DIM\": 768,                # <-- SBERT dimension\n",
    "        \"DISTANCE_METRIC\": \"COSINE\",\n",
    "        \"M\": 16,\n",
    "        \"EF_CONSTRUCTION\": 200\n",
    "    }, as_name=\"embedding\")\n",
    ")\n",
    "r.ft(INDEX_NAME).create_index(schema, definition=IndexDefinition(prefix=[\"search_cache:\"], index_type=IndexType.JSON))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "42f2d085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SBERT-based cache-first RAG helpers (stores embeddings as JSON float arrays; queries with float32 bytes)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np, re, time\n",
    "from redis.commands.search.query import Query\n",
    "\n",
    "embedder = SentenceTransformer('msmarco-distilbert-base-v4')  # 768-dim\n",
    "\n",
    "def embed_text(text: str) -> bytes:\n",
    "    # Return float32 bytes for querying (COSINE-friendly due to normalization)\n",
    "    v = embedder.encode(text, normalize_embeddings=True)\n",
    "    return np.asarray(v, dtype=np.float32).tobytes()\n",
    "\n",
    "def _knn(vec_bytes: bytes, k: int = 10):\n",
    "    q = (\n",
    "        Query(f'(*)=>[KNN {k} @embedding $v AS score]')\n",
    "        .sort_by(\"score\")\n",
    "        .return_fields(\"score\",\"title\",\"snippet\",\"domain\",\"lang\",\"fetched_at\",\"metadata\")\n",
    "        .dialect(2)\n",
    "    )\n",
    "    return r.ft(\"websearch_cache_idx\").search(q, query_params={\"v\": vec_bytes})\n",
    "\n",
    "def _count_hits(vec_bytes: bytes, k: int = 10, max_cosine_dist: float = 0.30) -> int:\n",
    "    res = _knn(vec_bytes, k=k)\n",
    "    return sum(1 for d in res.docs if float(d.score) <= max_cosine_dist)\n",
    "\n",
    "def _top_context(vec_bytes: bytes, k: int = 5, max_cosine_dist: float = 0.30):\n",
    "    print(f\"[DEBUG] Running cache KNN search for k={k}, threshold={max_cosine_dist}\")\n",
    "    res = _knn(vec_bytes, k=max(k, 20))\n",
    "    filtered = [d for d in res.docs if float(d.score) <= max_cosine_dist][:k]\n",
    "    if not filtered:\n",
    "        print(\"[DEBUG] No docs under threshold; falling back to top-k by distance\")\n",
    "        filtered = res.docs[:k]\n",
    "    items = []\n",
    "    for d in filtered:\n",
    "        m = re.search(r\"url=(\\S+)\", getattr(d, \"metadata\", \"\") or \"\")\n",
    "        items.append({\n",
    "            \"title\": getattr(d, \"title\", \"\"),\n",
    "            \"domain\": getattr(d, \"domain\", \"\"),\n",
    "            \"url\": m.group(1) if m else \"\",\n",
    "            \"snippet\": getattr(d, \"snippet\", \"\")\n",
    "        })\n",
    "    print(f\"[DEBUG] Retrieved {len(res.docs)} docs from cache; {len(items)} kept after threshold\")\n",
    "    return items\n",
    "\n",
    "def save_docs_with_embeddings(docs):\n",
    "    \"\"\"\n",
    "    Persist full schema to RedisJSON. Store embedding as a JSON array of float32s\n",
    "    (required for JSON-mode vector fields). Query vectors remain float32 bytes.\n",
    "    \"\"\"\n",
    "    print(f\"[DEBUG] Saving {len(docs)} docs to cache...\")\n",
    "    pipe = r.pipeline(transaction=False)\n",
    "    for i, src in enumerate(docs, 1):\n",
    "        text_for_embedding = f\"{src.get('title','')}\\n\\n{src.get('snippet','')}\".strip()\n",
    "        vec = embedder.encode(text_for_embedding, normalize_embeddings=True).astype(np.float32)\n",
    "        payload = {\n",
    "            \"query\":      src.get(\"query\", \"\"),\n",
    "            \"title\":      src.get(\"title\", \"\"),\n",
    "            \"snippet\":    (src.get(\"snippet\", \"\")[:4000]).strip(),\n",
    "            \"domain\":     src.get(\"domain\", \"\"),\n",
    "            \"lang\":       src.get(\"lang\", \"\"),\n",
    "            \"fetched_at\": int(src.get(\"fetched_at\") or time.time()),\n",
    "            \"metadata\":   src.get(\"metadata\", \"\"),\n",
    "            \"embedding\":  vec.tolist(),  # JSON-safe array of floats\n",
    "        }\n",
    "        key = f\"search_cache:{payload['fetched_at']}:{i}\"\n",
    "        pipe.json().set(key, \"$\", payload)\n",
    "    pipe.execute()\n",
    "    print(f\"[DEBUG] Saved {len(docs)} docs to cache\")\n",
    "\n",
    "def answer_with_cache_or_search(question: str, k: int = 5, threshold: float = 0.30):\n",
    "    # 1) Ask the LLM to craft a strong web search query\n",
    "    print(f\"[DEBUG] Incoming query: {question}\")\n",
    "    search_term = question\n",
    "    try:\n",
    "        q_resp = client_o3.chat.completions.create(\n",
    "            model=\"o3-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": (\n",
    "                    \"Rewrite the user's request as one concise, high-recall web search query. \"\n",
    "                    \"Prefer concrete nouns and key phrases; remove fluff; no quotes or commentary.\"\n",
    "                )},\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ],\n",
    "        )\n",
    "        cand = (q_resp.choices[0].message.content or \"\").strip()\n",
    "        if cand:\n",
    "            search_term = cand.splitlines()[0]\n",
    "    except Exception as e:\n",
    "        print(f\"[DEBUG] LLM query rewrite failed: {e} ‚Äî falling back to original question\")\n",
    "    print(f\"[DEBUG] Search term: {search_term}\")\n",
    "\n",
    "    # 2) Embed the search term and check cache by vector similarity\n",
    "    qvec = embed_text(search_term)\n",
    "    hit_count = _count_hits(qvec, k=20, max_cosine_dist=threshold)\n",
    "    print(f\"[DEBUG] Cache check: found {hit_count} matching docs (threshold={threshold})\")\n",
    "\n",
    "    if hit_count >= 3:\n",
    "        print(\"[DEBUG] Cache hit: retrieving from cache...\")\n",
    "        ctx = _top_context(qvec, k=k, max_cosine_dist=threshold)\n",
    "    else:\n",
    "        print(\"[DEBUG] Cache miss or insufficient hits ‚Äî running fresh web search...\")\n",
    "        fresh = web_search_google(search_term, k=k)\n",
    "        print(f\"[DEBUG] Web search returned {len(fresh)} docs\")\n",
    "        if fresh:\n",
    "            save_docs_with_embeddings(fresh)\n",
    "        ctx = _top_context(qvec, k=k, max_cosine_dist=threshold)\n",
    "\n",
    "    # Debug: list retrieved docs\n",
    "    print(f\"[DEBUG] Retrieved {len(ctx)} docs for RAG context:\")\n",
    "    for i, doc in enumerate(ctx, start=1):\n",
    "        print(f\"  [{i}] {doc['title']} ({doc['domain']})\")\n",
    "\n",
    "    # 4) Final answer with RAG context\n",
    "    system_prompt = \"You are a precise research assistant. Use ONLY the provided context. Cite domains inline. Be concise.\"\n",
    "    ctx_txt = \"\\n\\n\".join(f\"- {it['title']} ({it['domain']})\\n  {it['url']}\\n  {it['snippet'][:800]}\" for it in ctx)\n",
    "    user_msg = f\"Question: {question}\\nSearch term used: {search_term}\\n\\nContext:\\n{ctx_txt}\"\n",
    "\n",
    "    print(\"[DEBUG] Sending final prompt to LLM...\")\n",
    "    resp = client_o3.chat.completions.create(\n",
    "        model=\"o3-mini\",\n",
    "        messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                  {\"role\": \"user\", \"content\": user_msg}],\n",
    "    )\n",
    "    answer = resp.choices[0].message.content\n",
    "    print(\"[DEBUG] Received answer from LLM.\")\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "148bb215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RUN 1 ===\n",
      "[DEBUG] Incoming query: redis vector search tutorial\n",
      "[DEBUG] Search term: Redis vector search tutorial Redisearch beginner guide\n",
      "[DEBUG] Cache check: found 0 matching docs (threshold=0.3)\n",
      "[DEBUG] Cache miss or insufficient hits ‚Äî running fresh web search...\n",
      "[DEBUG] Web search returned 3 docs\n",
      "[DEBUG] Saving 3 docs to cache...\n",
      "[DEBUG] Saved 3 docs to cache\n",
      "[DEBUG] Running cache KNN search for k=5, threshold=0.3\n",
      "[DEBUG] No docs under threshold; falling back to top-k by distance\n",
      "[DEBUG] Retrieved 3 docs from cache; 3 kept after threshold\n",
      "[DEBUG] Retrieved 3 docs for RAG context:\n",
      "  [1] Redis Vector Store | ü¶úÔ∏èüîó LangChain (python.langchain.com)\n",
      "  [2] Redis Vector Database: A Comprehensive Guide - Flutters (flutters.in)\n",
      "  [3] Valkey ¬∑ Introducing Vector Search To Valkey (valkey.io)\n",
      "[DEBUG] Sending final prompt to LLM...\n",
      "[DEBUG] Received answer from LLM.\n",
      "\n",
      "Query: redis vector search tutorial\n",
      "Time: 30.85s\n",
      "Answer preview:\n",
      "For an introductory Redis vector search tutorial, consider starting with the notebook on Redis Vector Store by LangChain (python.langchain.com). It walks you through setting up Redis with vector similarity search capabilities and demonstrates performing vector queries using embeddings. Additionally, the comprehensive guide on Redis Vector Database (flutters.in) explains how Redis, via Redis Stack, functions as a vector database‚Äîideal for semantic search and recommendation systems. For more context on vector search concepts, you might also review the Valkey article on introducing vector search (valkey.io), which offers insights into indexing and high-performance similarity search, although it\n",
      "[DEBUG] Incoming query: FT.SEARCH KNN example in Redis\n",
      "[DEBUG] Search term: Redis FT.SEARCH KNN example Redis vector search FT.SEARCH KNN usage\n",
      "[DEBUG] Cache check: found 0 matching docs (threshold=0.3)\n",
      "[DEBUG] Cache miss or insufficient hits ‚Äî running fresh web search...\n",
      "[DEBUG] Web search returned 5 docs\n",
      "[DEBUG] Saving 5 docs to cache...\n",
      "[DEBUG] Saved 5 docs to cache\n",
      "[DEBUG] Running cache KNN search for k=5, threshold=0.3\n",
      "[DEBUG] No docs under threshold; falling back to top-k by distance\n",
      "[DEBUG] Retrieved 8 docs from cache; 5 kept after threshold\n",
      "[DEBUG] Retrieved 5 docs for RAG context:\n",
      "  [1] kNN search | Elastic Docs (elastic.co)\n",
      "  [2] Redis Vector Store | ü¶úÔ∏èüîó LangChain (python.langchain.com)\n",
      "  [3] Redis Vector Store | ü¶úÔ∏èüîó LangChain (python.langchain.com)\n",
      "  [4] Redis Vector Database: A Comprehensive Guide - Flutters (flutters.in)\n",
      "  [5] Redis Vector Similarity Search (artefactory.github.io)\n",
      "[DEBUG] Sending final prompt to LLM...\n",
      "[DEBUG] Received answer from LLM.\n",
      "\n",
      "Query: FT.SEARCH KNN example in Redis\n",
      "Time: 31.13s\n",
      "Answer preview:\n",
      "Below is one example of how you might use FT.SEARCH with a kNN clause in Redis. Assume you‚Äôve already created an index with a vector field (for example, named ‚Äúembedding‚Äù) and stored documents that include vector embeddings. The following query finds the three (k = 3) nearest neighbors to the provided query vector:\n",
      "\n",
      "‚ÄÉ‚ÄÉFT.SEARCH vectorIdx \"*=>[KNN 3 @embedding $query_vector]\" RETURN 1 title PARAMS 2 query_vector \"[BINARY_VECTOR_DATA]\" DIALECT 2\n",
      "\n",
      "Key points:\n",
      "‚Ä¢ The index ‚ÄúvectorIdx‚Äù must have a vector field ‚Äúembedding‚Äù (indexed as a VECTOR field via FLAT or HNSW, for example).\n",
      "‚Ä¢ The query clause ‚Äú*=>[KNN 3 @embedding $query_vector]‚Äù tells Redis to find the 3 nearest neighbors, comparing ‚Äúembedd\n",
      "[DEBUG] Incoming query: Redis JSON + RediSearch schema for vector embeddings\n",
      "[DEBUG] Search term: Redis JSON RediSearch schema vector embeddings\n",
      "[DEBUG] Cache check: found 0 matching docs (threshold=0.3)\n",
      "[DEBUG] Cache miss or insufficient hits ‚Äî running fresh web search...\n",
      "[DEBUG] Web search returned 5 docs\n",
      "[DEBUG] Saving 5 docs to cache...\n",
      "[DEBUG] Saved 5 docs to cache\n",
      "[DEBUG] Running cache KNN search for k=5, threshold=0.3\n",
      "[DEBUG] No docs under threshold; falling back to top-k by distance\n",
      "[DEBUG] Retrieved 10 docs from cache; 5 kept after threshold\n",
      "[DEBUG] Retrieved 5 docs for RAG context:\n",
      "  [1] Redis Vector Database: A Comprehensive Guide - Flutters (flutters.in)\n",
      "  [2] Redis Vector Database: A Comprehensive Guide - Flutters (flutters.in)\n",
      "  [3] Efficient vector similarity search with Redis: a step-by-step tutorial tutorial (lablab.ai)\n",
      "  [4] Redis Vector Store | ü¶úÔ∏èüîó LangChain (python.langchain.com)\n",
      "  [5] Redis Vector Store | ü¶úÔ∏èüîó LangChain (python.langchain.com)\n",
      "[DEBUG] Sending final prompt to LLM...\n",
      "[DEBUG] Received answer from LLM.\n",
      "\n",
      "Query: Redis JSON + RediSearch schema for vector embeddings\n",
      "Time: 29.49s\n",
      "Answer preview:\n",
      "You can combine Redis JSON documents (holding your vector embeddings) with a RediSearch index by storing the embeddings as a JSON field and then creating an index that maps JSON paths to schema fields. For example, you might store your vector and metadata in a JSON document like this:\n",
      "\n",
      "‚ÄÉ‚ÄÉ{\n",
      "‚ÄÉ‚ÄÉ‚ÄÉ\"text\": \"document text\",\n",
      "‚ÄÉ‚ÄÉ‚ÄÉ\"vector\": [0.12, 0.35, ‚Ä¶]‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ // your embedding vector\n",
      "‚ÄÉ‚ÄÉ}\n",
      "\n",
      "Then you create a RediSearch index using the FT.CREATE command on JSON documents. The schema includes a pointer to the text field and another pointer to the vector field with vector-specific options. A sample command might be:\n",
      "\n",
      "‚ÄÉ‚ÄÉFT.CREATE idx:docs ON JSON SCHEMA \n",
      "‚ÄÉ‚ÄÉ‚ÄÉ\"$.text\" AS text TEXT \n",
      "‚ÄÉ‚ÄÉ‚ÄÉ\"$.vector\" AS vector VEC\n",
      "\n",
      "=== RUN 2 ===\n",
      "[DEBUG] Incoming query: redis vector search tutorial\n",
      "[DEBUG] Search term: redis vector search tutorial guide documentation example\n",
      "[DEBUG] Cache check: found 0 matching docs (threshold=0.3)\n",
      "[DEBUG] Cache miss or insufficient hits ‚Äî running fresh web search...\n",
      "[DEBUG] Web search returned 5 docs\n",
      "[DEBUG] Saving 5 docs to cache...\n",
      "[DEBUG] Saved 5 docs to cache\n",
      "[DEBUG] Running cache KNN search for k=5, threshold=0.3\n",
      "[DEBUG] No docs under threshold; falling back to top-k by distance\n",
      "[DEBUG] Retrieved 10 docs from cache; 5 kept after threshold\n",
      "[DEBUG] Retrieved 5 docs for RAG context:\n",
      "  [1] Redis Vector Database: A Comprehensive Guide - Flutters (flutters.in)\n",
      "  [2] Redis Vector Database: A Comprehensive Guide - Flutters (flutters.in)\n",
      "  [3] Redis Vector Search Integration ‚Äî FiftyOne 1.7.2 documentation (docs.voxel51.com)\n",
      "  [4] Redis Vector Store | ü¶úÔ∏èüîó LangChain (python.langchain.com)\n",
      "  [5] Redis Vector Store | ü¶úÔ∏èüîó LangChain (python.langchain.com)\n",
      "[DEBUG] Sending final prompt to LLM...\n",
      "[DEBUG] Received answer from LLM.\n",
      "\n",
      "Query: redis vector search tutorial\n",
      "Time: 31.80s\n",
      "Answer preview:\n",
      "Redis offers several tutorials and guides that help you get started with vector search using Redis. For example, one guide from flutters.in provides a comprehensive overview‚Äîincluding a quick start guide and an explanation of how Redis evolved into a vector database‚Äîideal for learning how to store and query vector embeddings (flutters.in). Additionally, if you're working with computer vision or similar data, the FiftyOne documentation shows how to configure Redis and integrate it into your workflow, complete with instructions to create vector indexes and run similarity queries via Python or a point‚Äêand‚Äêclick interface (docs.voxel51.com). Lastly, LangChain offers a notebook tutorial that cove\n",
      "[DEBUG] Incoming query: FT.SEARCH KNN example in Redis\n",
      "[DEBUG] Search term: Redis RediSearch FT.SEARCH KNN example usage Redis documentation Redis module example\n",
      "[DEBUG] Cache check: found 0 matching docs (threshold=0.3)\n",
      "[DEBUG] Cache miss or insufficient hits ‚Äî running fresh web search...\n",
      "[DEBUG] Web search returned 5 docs\n",
      "[DEBUG] Saving 5 docs to cache...\n",
      "[DEBUG] Saved 5 docs to cache\n",
      "[DEBUG] Running cache KNN search for k=5, threshold=0.3\n",
      "[DEBUG] No docs under threshold; falling back to top-k by distance\n",
      "[DEBUG] Retrieved 10 docs from cache; 5 kept after threshold\n",
      "[DEBUG] Retrieved 5 docs for RAG context:\n",
      "  [1] What is a Redis Search? | Redisson (redisson.pro)\n",
      "  [2] Redis ‚Äî ü¶úüîó LangChain documentation (python.langchain.com)\n",
      "  [3] Redis Vector Store | ü¶úÔ∏èüîó LangChain (python.langchain.com)\n",
      "  [4] Redis Vector Store | ü¶úÔ∏èüîó LangChain (python.langchain.com)\n",
      "  [5] Redis Vector Store | ü¶úÔ∏èüîó LangChain (python.langchain.com)\n",
      "[DEBUG] Sending final prompt to LLM...\n",
      "[DEBUG] Received answer from LLM.\n",
      "\n",
      "Query: FT.SEARCH KNN example in Redis\n",
      "Time: 28.21s\n",
      "Answer preview:\n",
      "A common example of performing a k‚Äënearest neighbors (KNN) search using RediSearch looks like this:\n",
      "\n",
      "‚ÄÉ‚ÄÉFT.SEARCH myIndex \"*=>[KNN 10 @vector $vec]\" SORTBY __vector_score ASC PARAMS 2 vec <binary_vector> DIALECT 2\n",
      "\n",
      "In this command:\n",
      "‚Ä¢ \"myIndex\" is the index on which the search is performed.\n",
      "‚Ä¢ \"*=>[KNN 10 @vector $vec]\" specifies that you want the 10 nearest neighbors based on the field \"vector\".\n",
      "‚Ä¢ The sort ‚ÄúSORTBY __vector_score ASC‚Äù orders based on the computed similarity.\n",
      "‚Ä¢ \"PARAMS 2 vec <binary_vector>\" passes your query vector as a parameter.\n",
      "‚Ä¢ DIALECT 2 activates the newer query parser.\n",
      "\n",
      "This example, which utilizes vector similarity search (a feature noted in Redis Stack documentation as\n",
      "[DEBUG] Incoming query: Redis JSON + RediSearch schema for vector embeddings\n",
      "[DEBUG] Search term: Redis JSON RediSearch schema vector embeddings tutorial\n",
      "[DEBUG] Cache check: found 0 matching docs (threshold=0.3)\n",
      "[DEBUG] Cache miss or insufficient hits ‚Äî running fresh web search...\n",
      "[DEBUG] Web search returned 5 docs\n",
      "[DEBUG] Saving 5 docs to cache...\n",
      "[DEBUG] Saved 5 docs to cache\n",
      "[DEBUG] Running cache KNN search for k=5, threshold=0.3\n",
      "[DEBUG] No docs under threshold; falling back to top-k by distance\n",
      "[DEBUG] Retrieved 10 docs from cache; 5 kept after threshold\n",
      "[DEBUG] Retrieved 5 docs for RAG context:\n",
      "  [1] Efficient vector similarity search with Redis: a step-by-step tutorial tutorial (lablab.ai)\n",
      "  [2] Efficient vector similarity search with Redis: a step-by-step tutorial tutorial (lablab.ai)\n",
      "  [3] Redis Vector Database: A Comprehensive Guide - Flutters (flutters.in)\n",
      "  [4] Redis Vector Database: A Comprehensive Guide - Flutters (flutters.in)\n",
      "  [5] Redis Vector Database: A Comprehensive Guide - Flutters (flutters.in)\n",
      "[DEBUG] Sending final prompt to LLM...\n",
      "[DEBUG] Received answer from LLM.\n",
      "\n",
      "Query: Redis JSON + RediSearch schema for vector embeddings\n",
      "Time: 31.74s\n",
      "Answer preview:\n",
      "A common approach is to store your documents as JSON and include a field holding the vector embedding. Then, using RediSearch you define an index on those JSON documents‚Äîwith a schema that maps standard text fields (if needed) along with a vector field. For example, one might structure the schema definition as follows (adjust the details as needed):\n",
      "\n",
      "‚Ä¢ Store each document in RedisJSON with keys like:\n",
      "‚ÄÉ{ \"id\": \"doc:1\", \"title\": \"Example Title\", \"embedding\": [0.12, 0.34, ‚Ä¶] }\n",
      "\n",
      "‚Ä¢ Create a RediSearch index on JSON documents with a command such as:\n",
      "‚ÄÉFT.CREATE idx ON JSON SCHEMA \n",
      "‚ÄÉ‚ÄÉ$.title AS title TEXT \n",
      "‚ÄÉ‚ÄÉ$.embedding AS embedding VECTOR FLOAT32 128 DIM 128 DISTANCE_METRIC COSINE\n",
      "\n",
      "In this example:\n"
     ]
    }
   ],
   "source": [
    "# Quick smoke test for the cache-first RAG flow\n",
    "sample_queries = [\n",
    "    \"redis vector search tutorial\",\n",
    "    \"FT.SEARCH KNN example in Redis\",\n",
    "    \"Redis JSON + RediSearch schema for vector embeddings\"\n",
    "]\n",
    "\n",
    "for run in (1, 2):  # run twice to observe cache usage on the second pass\n",
    "    print(f\"\\n=== RUN {run} ===\")\n",
    "    for q in sample_queries:\n",
    "        t0 = time.time()\n",
    "        answer = answer_with_cache_or_search(q, k=5, threshold=0.30)\n",
    "        dt = time.time() - t0\n",
    "        print(f\"\\nQuery: {q}\\nTime: {dt:.2f}s\\nAnswer preview:\\n{answer[:700]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "70d4be21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RUN 1 ===\n",
      "[DEBUG] Incoming query: redis vector search tutorial\n",
      "[DEBUG] Search term: Redis vector search tutorial examples and guides\n",
      "[DEBUG] Cache check: found 10 matching docs (threshold=0.55)\n",
      "[DEBUG] Cache hit: retrieving from cache...\n",
      "[DEBUG] Running cache KNN search for k=5, threshold=0.55\n",
      "[DEBUG] Retrieved 10 docs from cache; 5 kept after threshold\n",
      "[DEBUG] Retrieved 5 docs for RAG context:\n",
      "  [1] Vector search | Docs (redis-docs.ru)\n",
      "  [2] Redis Vector Database: A Comprehensive Guide - Flutters (flutters.in)\n",
      "  [3] Redis Vector Database: A Comprehensive Guide - Flutters (flutters.in)\n",
      "  [4] Redis Vector Database: A Comprehensive Guide - Flutters (flutters.in)\n",
      "  [5] Redis Vector Store | ü¶úÔ∏èüîó LangChain (python.langchain.com)\n",
      "[DEBUG] Sending final prompt to LLM...\n",
      "[DEBUG] Received answer from LLM.\n",
      "\n",
      "Query: redis vector search tutorial\n",
      "Time: 7.45s\n",
      "Answer preview:\n",
      "You can find several tutorials and examples on performing vector search with Redis by checking these resources:\n",
      "\n",
      "‚Ä¢ The Redis documentation (redis-docs.ru) explains how to use vector fields with the FT.SEARCH command for k-nearest neighbor (KNN) queries and radius searches. This guide details schema creation and parameter settings for querying vector embeddings. (redis-docs.ru)\n",
      "\n",
      "‚Ä¢ Flutters provides a comprehensive guide on using Redis as a vector database, including quick start instructions and use cases for similarity searches in recommendation systems and semantic search engines. (flutters.in)\n",
      "\n",
      "‚Ä¢ LangChain‚Äôs documentation for the Redis Vector Store offers a notebook-style introduction, show\n",
      "[DEBUG] Incoming query: FT.SEARCH KNN example in Redis\n",
      "[DEBUG] Search term: Redis FT.SEARCH KNN example query usage\n",
      "[DEBUG] Cache check: found 4 matching docs (threshold=0.55)\n",
      "[DEBUG] Cache hit: retrieving from cache...\n",
      "[DEBUG] Running cache KNN search for k=5, threshold=0.55\n",
      "[DEBUG] Retrieved 10 docs from cache; 4 kept after threshold\n",
      "[DEBUG] Retrieved 4 docs for RAG context:\n",
      "  [1] kNN search | Elastic Docs (elastic.co)\n",
      "  [2] Vector search | Docs (redis-docs.ru)\n",
      "  [3] Hash vs JSON Storage ‚Äî RedisVL (docs.redisvl.com)\n",
      "  [4] Hash vs JSON Storage ‚Äî RedisVL (docs.redisvl.com)\n",
      "[DEBUG] Sending final prompt to LLM...\n",
      "[DEBUG] Received answer from LLM.\n",
      "\n",
      "Query: FT.SEARCH KNN example in Redis\n",
      "Time: 8.80s\n",
      "Answer preview:\n",
      "A typical kNN query using FT.SEARCH in Redis looks like this:\n",
      "\n",
      "‚ÄÉ‚ÄÉFT.SEARCH idx \"*=>[KNN 10 @vector $query]\" RETURN 2 id __vector_score\n",
      "\n",
      "Here:\n",
      "‚Ä¢ idx is your index name.\n",
      "‚Ä¢ The \"*=>[KNN 10 @vector $query]\" part tells Redis to search for the 10 nearest neighbors (k=10) based on the vector stored in the field ‚Äúvector.‚Äù The query vector is provided as a parameter ($query).\n",
      "‚Ä¢ RETURN 2 id __vector_score instructs Redis to return the document id and its computed distance or similarity score.\n",
      "\n",
      "Before running such a query, ensure you‚Äôve defined your schema so that the ‚Äúvector‚Äù field uses a dense_vector type and that your vectors are generated consistently (for example, with the same NLP model). This ap\n",
      "[DEBUG] Incoming query: Redis JSON + RediSearch schema for vector embeddings\n",
      "[DEBUG] Search term: Redis JSON RediSearch schema vector embeddings configuration example\n",
      "[DEBUG] Cache check: found 10 matching docs (threshold=0.55)\n",
      "[DEBUG] Cache hit: retrieving from cache...\n",
      "[DEBUG] Running cache KNN search for k=5, threshold=0.55\n",
      "[DEBUG] Retrieved 10 docs from cache; 5 kept after threshold\n",
      "[DEBUG] Retrieved 5 docs for RAG context:\n",
      "  [1] Redis Modules - coredis {5.0.1} (coredis.readthedocs.io)\n",
      "  [2] Redis Vector Database: A Comprehensive Guide - Flutters (flutters.in)\n",
      "  [3] Redis Vector Database: A Comprehensive Guide - Flutters (flutters.in)\n",
      "  [4] Redis Vector Database: A Comprehensive Guide - Flutters (flutters.in)\n",
      "  [5] Vector search | Docs (redis-docs.ru)\n",
      "[DEBUG] Sending final prompt to LLM...\n",
      "[DEBUG] Received answer from LLM.\n",
      "\n",
      "Query: Redis JSON + RediSearch schema for vector embeddings\n",
      "Time: 10.06s\n",
      "Answer preview:\n",
      "Below is an example of how you might define a RedisJSON document and create a RediSearch index that includes a vector embedding field. In this example, the document stores both a text field and a vector field (an array of floats), and the index schema defines the vector field with the VECTOR type using (for example) the HNSW algorithm. (See coredis.readthedocs.io for module usage, flutters.in for vector database concepts, and redis-docs.ru for vector search details.)\n",
      "\n",
      "1. Insert a JSON document (using RedisJSON):\n",
      "\n",
      "‚ÄÉ‚ÄÉJSON.SET doc:1 $ '{\"text\": \"Sample text\", \"vector\": [0.12, 0.45, 0.67, ‚Ä¶] }'\n",
      "\n",
      "2. Create an index with RediSearch on JSON documents with a vector field. Adjust parameters (vector d\n",
      "\n",
      "=== RUN 2 ===\n",
      "[DEBUG] Incoming query: redis vector search tutorial\n",
      "[DEBUG] Search term: Redis vector search tutorial guide documentation best practices indexing similarity search integration Redis AI\n",
      "[DEBUG] Cache check: found 10 matching docs (threshold=0.55)\n",
      "[DEBUG] Cache hit: retrieving from cache...\n",
      "[DEBUG] Running cache KNN search for k=5, threshold=0.55\n",
      "[DEBUG] Retrieved 10 docs from cache; 5 kept after threshold\n",
      "[DEBUG] Retrieved 5 docs for RAG context:\n",
      "  [1] Redis Vector Search Integration ‚Äî FiftyOne 1.7.2 documentation (docs.voxel51.com)\n",
      "  [2] Efficient vector similarity search with Redis: a step-by-step tutorial tutorial (lablab.ai)\n",
      "  [3] Efficient vector similarity search with Redis: a step-by-step tutorial tutorial (lablab.ai)\n",
      "  [4] Redis Vector Store | ü¶úÔ∏èüîó LangChain (python.langchain.com)\n",
      "  [5] Redis Vector Store | ü¶úÔ∏èüîó LangChain (python.langchain.com)\n",
      "[DEBUG] Sending final prompt to LLM...\n",
      "[DEBUG] Received answer from LLM.\n",
      "\n",
      "Query: redis vector search tutorial\n",
      "Time: 6.84s\n",
      "Answer preview:\n",
      "You can start by exploring the Redis vector search integration provided by FiftyOne, which walks you through configuring a local Redis server, uploading embeddings, and running similarity queries‚Äîboth via an API and the FiftyOne App (docs.voxel51.com). Additionally, a detailed step-by-step tutorial on efficient vector similarity search with Redis is available on lablab.ai, which explains how to leverage deep learning‚Äìgenerated embeddings to improve search relevance (lablab.ai). Finally, if you‚Äôre working with LangChain, there‚Äôs a guide on setting up the Redis vector store that covers integration details and usage scenarios (python.langchain.com).\n",
      "[DEBUG] Incoming query: FT.SEARCH KNN example in Redis\n",
      "[DEBUG] Search term: Redis FT.SEARCH KNN example usage tutorial documentation\n",
      "[DEBUG] Cache check: found 1 matching docs (threshold=0.55)\n",
      "[DEBUG] Cache miss or insufficient hits ‚Äî running fresh web search...\n",
      "[DEBUG] Web search returned 5 docs\n",
      "[DEBUG] Saving 5 docs to cache...\n",
      "[DEBUG] Saved 5 docs to cache\n",
      "[DEBUG] Running cache KNN search for k=5, threshold=0.55\n",
      "[DEBUG] Retrieved 10 docs from cache; 2 kept after threshold\n",
      "[DEBUG] Retrieved 2 docs for RAG context:\n",
      "  [1] kNN search | Elastic Docs (elastic.co)\n",
      "  [2] kNN search | Elastic Docs (elastic.co)\n",
      "[DEBUG] Sending final prompt to LLM...\n",
      "[DEBUG] Received answer from LLM.\n",
      "\n",
      "Query: FT.SEARCH KNN example in Redis\n",
      "Time: 33.38s\n",
      "Answer preview:\n",
      "An example of running a k‚Äënearest neighbor search with Redis‚Äô FT.SEARCH looks roughly like this:\n",
      "\n",
      "‚ÄÉFT.SEARCH myIndex \"*=>[KNN $K @vector $query]\"  \n",
      "‚ÄÉPARAMS 2 K 5 query [‚Ä¶vector values‚Ä¶]  \n",
      "‚ÄÉRETURN 3 field1 field2 field3\n",
      "\n",
      "In this example:\n",
      "‚Ä¢ The index ‚ÄúmyIndex‚Äù contains documents that have a vector field (named ‚Äúvector‚Äù).  \n",
      "‚Ä¢ The query uses the ‚Äú*=>[KNN $K @vector $query]‚Äù syntax to request the 5 (that is, K = 5) nearest vectors to the given query vector.\n",
      "‚Ä¢ The ‚ÄúPARAMS‚Äù clause provides the actual k (here, 5) and the query vector.  \n",
      "‚Ä¢ The ‚ÄúRETURN‚Äù clause specifies additional fields to return from each matching document.\n",
      "\n",
      "Like in Elastic‚Äôs kNN examples (elastic.co), ensure your data is preprocess\n",
      "[DEBUG] Incoming query: Redis JSON + RediSearch schema for vector embeddings\n",
      "[DEBUG] Search term: Redis JSON RediSearch schema vector embeddings example\n",
      "[DEBUG] Cache check: found 10 matching docs (threshold=0.55)\n",
      "[DEBUG] Cache hit: retrieving from cache...\n",
      "[DEBUG] Running cache KNN search for k=5, threshold=0.55\n",
      "[DEBUG] Retrieved 10 docs from cache; 5 kept after threshold\n",
      "[DEBUG] Retrieved 5 docs for RAG context:\n",
      "  [1] Redis Modules - coredis {5.0.1} (coredis.readthedocs.io)\n",
      "  [2] Redis Vector Database: A Comprehensive Guide - Flutters (flutters.in)\n",
      "  [3] Redis Vector Database: A Comprehensive Guide - Flutters (flutters.in)\n",
      "  [4] Redis Vector Database: A Comprehensive Guide - Flutters (flutters.in)\n",
      "  [5] Vector search | Docs (redis-docs.ru)\n",
      "[DEBUG] Sending final prompt to LLM...\n",
      "[DEBUG] Received answer from LLM.\n",
      "\n",
      "Query: Redis JSON + RediSearch schema for vector embeddings\n",
      "Time: 10.80s\n",
      "Answer preview:\n",
      "Below is one example of how you might combine Redis JSON documents with a RediSearch index for vector embeddings. In this example, your JSON documents are stored under keys with a common prefix (for example, \"doc:\") that include both standard text fields and a vector field (for instance, at JSON path $.embedding). You can then define your index using FT.CREATE with the ON JSON option and a schema that maps JSON paths to fields. For example:\n",
      "\n",
      "‚ÄÉ‚ÄÉFT.CREATE myIndex ON JSON PREFIX 1 \"doc:\" SCHEMA\n",
      "‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ$.title AS title TEXT\n",
      "‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ$.embedding AS embedding VECTOR FLAT 128 TYPE FLOAT32 DIM 128 DISTANCE_METRIC COSINE INITIAL_CAP 100\n",
      "\n",
      "In this schema:\n",
      "‚Ä¢ The title field is indexed as a TEXT field.\n",
      "‚Ä¢ The emb\n"
     ]
    }
   ],
   "source": [
    "# Quick smoke test for the cache-first RAG flow\n",
    "sample_queries = [\n",
    "    \"redis vector search tutorial\",\n",
    "    \"FT.SEARCH KNN example in Redis\",\n",
    "    \"Redis JSON + RediSearch schema for vector embeddings\"\n",
    "]\n",
    "\n",
    "for run in (1, 2):  # run twice to observe cache usage on the second pass\n",
    "    print(f\"\\n=== RUN {run} ===\")\n",
    "    for q in sample_queries:\n",
    "        t0 = time.time()\n",
    "        answer = answer_with_cache_or_search(q, k=5, threshold=0.55)\n",
    "        dt = time.time() - t0\n",
    "        print(f\"\\nQuery: {q}\\nTime: {dt:.2f}s\\nAnswer preview:\\n{answer[:700]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
